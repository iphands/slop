# llama-proxy configuration
# Copy to config.yaml and modify as needed

# Proxy server settings
server:
  # Port to listen on
  port: 8066
  # Bind address
  host: "0.0.0.0"

# Backend llama-server configuration
backend:
  # Full URL to llama-server (supports http:// and https://)
  url: "http://localhost:8080"
  # Request timeout in seconds
  timeout_seconds: 300
  # TLS configuration (optional, for https:// backends)
  # tls:
  #   # Accept invalid certificates (self-signed, expired)
  #   accept_invalid_certs: false
  #   # Path to custom CA certificate (PEM format)
  #   ca_cert_path: "/path/to/ca.pem"
  #   # Path to client certificate for mTLS
  #   client_cert_path: "/path/to/client.pem"
  #   # Path to client private key for mTLS
  #   client_key_path: "/path/to/client.key"

# Response fix modules (enable/disable)
fixes:
  # Enable all fixes by default
  enabled: true
  # Individual fix toggles
  modules:
    # Fix null or missing index fields in tool calls
    # llama.cpp sometimes sends index=null which causes client validation errors
    toolcall_null_index:
      enabled: true

    # Fix duplicate/malformed filePath in Qwen3-Coder tool calls
    toolcall_bad_filepath:
      enabled: true
      # If true, remove duplicate keys; if false, fix and keep both
      remove_duplicate: true

    # Fix malformed tool call arguments with invalid property names like `{}":`
    # Uses tool schemas from request to determine correct parameter names
    toolcall_malformed_arguments:
      enabled: true

# Pre-parse detection of malformed patterns
# This runs BEFORE JSON parsing and logs warnings immediately
detection:
  # Enable pre-parse malformed pattern detection
  enabled: true
  # Log level for detections: "warn" | "error" | "info"
  log_level: "warn"

# Streaming control
streaming:
  # Enable streaming responses globally
  # Set to false to force all requests to use non-streaming
  enabled: true

  # Allow streaming when request contains tools
  # Set to false to disable streaming for tool requests
  streaming_on_tools: true

  # Per-client streaming rules (optional)
  # Map User-Agent substrings to streaming enabled/disabled
  # Example:
  #   client_rules:
  #     "claude-code": false
  #     "opencode": true
  client_rules: {}

# Stats logging to STDOUT
stats:
  # Enable stats logging
  enabled: true
  # Log format: "pretty" | "json" | "compact"
  format: "pretty"
  # Log every N requests (1 = every request)
  log_interval: 1

# Remote exporters for metrics
exporters:
  # InfluxDB v2 exporter
  influxdb:
    enabled: false
    # InfluxDB server URL
    url: "http://localhost:8086"
    # Organization
    org: "my-org"
    # Bucket name
    bucket: "llama-metrics"
    # Authentication token
    token: "your-token-here"
    # Batch writes (0 = immediate)
    batch_size: 10
    # Flush interval in seconds
    flush_interval_seconds: 5
