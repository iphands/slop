# llama-proxy configuration
# Copy to config.yaml and modify as needed

# === REQUIRED SECTIONS ===

# Proxy server settings
server:
  # Port to listen on
  port: 8066
  # Bind address
  host: "0.0.0.0"

# Backend llama-server configuration
backend:
  # Full URL to llama-server (supports http:// and https://)
  url: "http://localhost:8080"
  # Request timeout in seconds
  timeout_seconds: 300
  # TLS configuration (optional, for https:// backends)
  # tls:
  #   # Accept invalid certificates (self-signed, expired)
  #   accept_invalid_certs: false
  #   # Path to custom CA certificate (PEM format)
  #   ca_cert_path: "/path/to/ca.pem"
  #   # Path to client certificate for mTLS
  #   client_cert_path: "/path/to/client.pem"
  #   # Path to client private key for mTLS
  #   client_key_path: "/path/to/client.key"

# === OPTIONAL SECTIONS (defaults shown) ===

# Response fix modules (default: enabled with all fixes active)
# fixes:
#   enabled: true
#   modules:
#     toolcall_null_index:
#       enabled: true
#     toolcall_bad_filepath:
#       enabled: true
#       remove_duplicate: true
#     toolcall_malformed_arguments:
#       enabled: true

# Pre-parse detection (default: enabled with warn level)
# detection:
#   enabled: true
#   log_level: "warn"

# Streaming mode (default: fake)
# Controls how the proxy handles streaming responses:
#   disabled: Forces streaming off completely
#   fake: Forces non-streaming to backend, synthesizes streaming to frontend
#   accumulator: NOT IMPLEMENTED - will error if used
# streaming: fake

# Stats logging (default: enabled, pretty format, log every request)
# stats:
#   enabled: true
#   format: "pretty"
#   log_interval: 1

# Remote exporters (default: all disabled)
# exporters:
#   influxdb:
#     enabled: false
#     url: "http://localhost:8086"
#     org: "my-org"
#     bucket: "llama-metrics"
#     token: "your-token-here"
#     batch_size: 10
#     flush_interval_seconds: 5
